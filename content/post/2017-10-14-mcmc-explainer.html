---
title: "MCMC and the case of the spilled seeds"
author: "Nick Strayer"
date: '2017-10-14'
categories: ["interactive", "algorithms", "visualization"]
tags: ["interactive", "algorithms", "visualization"]
excerpt: "For a long time I was confused by MCMC. I didn't understand what it was, how it worked, and why we needed to do it. In this post I attempt to clear up those questions and allow you to play with the Metroplis Haystings algorithm as it attempts to find a posterior to help solve a mystery of two messy birds."
---



<p>Earlier this month I did a post on simulated annealing, an algorithm that I learned in the class I’m currently taking: <a href="http://stronginference.com/Bios8366/">Advanced Statistical Computing</a> taught by <a href="https://twitter.com/fonnesbeck">Chris Fonnesbeck</a> here at Vanderbilt. Recently we have moved on to more traditionally “statistics” algorithms, and one that Dr. Fonnesbeck is particularly well versed in, Markov-Chain Monte-Carlo (or MCMC). <label for="tufte-mn-gwjzbrvyno" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-tufte-mn-gwjzbrvyno" class="margin-toggle"><span class="marginnote">Dr. Fonnesbeck is the creator of the very popular python library <a href="http://docs.pymc.io/">pymc</a>.</span> In this post I hope to elucidate what MCMC is, why we need it, and show how one particular way of doing it (the metropolis hastings sampler) works.</p>
<div id="what-is-mcmc" class="section level2">
<h2>What is MCMC?</h2>
<p>If you want to sound smart, next time someone asks you what you like thinking about, say “Markov-Chain Monte-Carlo”, <label for="tufte-mn-jpnipiderw" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-tufte-mn-jpnipiderw" class="margin-toggle"><span class="marginnote"><img src="http://nbwsblog.com/wp-content/uploads/2017/08/Marco-Polo.gif" /> I always think of Marco Polo when I think of Monte-Carlo.</span> it’s a nice long mouthful of sciency soundings words that cause most people to zone out after about two syllables. What does it mean in normal language? <strong>Markov-Chain</strong> <label for="tufte-mn-ddaazgnbkr" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-tufte-mn-ddaazgnbkr" class="margin-toggle"><span class="marginnote"><img src="https://cdn-images-1.medium.com/max/2000/1*my_udGVBPa4U5L6U_9m81Q.png" /> Markov Chains are used in lots of cool places. For instance, <a href="https://chatbotslife.com/notes-on-remixing-noon-generative-text-and-markov-chains-84ff4ec23937">generating fake text.</a></span>refers to the idea of a sequence of draws such that the only thing that all you need to know about what the next value will be is the one just prior to it. Think of it like the mathematical equivalent to Dory from Finding Nemo, once she’s in a place she has no memory of where she was before and her next action is based only upon her current situation. <strong>Monte-Carlo</strong> refers to the act of estimating something by simulation. Basically the concept of, if you want to understand a process, repeat it a bunch of times and look at the aggregate behavior. <label for="tufte-mn-effwxwommu" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-tufte-mn-effwxwommu" class="margin-toggle"><span class="marginnote">Another example of monte carlo methods in statistics can be seen in my previous post <a href="http://livefreeordichotomize.com/2016/12/24/wait-what-are-p-values/">Wait, what are P-values?</a> </span></p>
</div>
<div id="why-do-we-need-mcmc" class="section level2">
<h2>Why do we need MCMC?</h2>
<p>When I say ‘we’ in this context I mean Bayesian statisticians (which, while reading this article, you are), or statisticians that like to think in terms of probabilities instead of frequencies. I won’t go into the Bayesian vs Frequentest debate here as it would be way too long and boring, but there’s plenty of literature on it to be found. <label for="tufte-mn-erjjdikmys" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-tufte-mn-erjjdikmys" class="margin-toggle"><span class="marginnote">Like all of life’s problems, stack overflow has a <a href="https://stats.stackexchange.com/questions/22/bayesian-and-frequentist-reasoning-in-plain-english">good answer.</a></span></p>
<p>A huge reason why Bayesian statistics was so unpopular up until very recently is because when calculating the “posterior” distribution a large portion of the time the math simply wasn’t tractable. To see what I mean it helps to think about the form of a posterior distribution.</p>
<p><span class="math display">\[\begin{aligned}
\text{Posterior of parameter} &amp;= \frac{P(\text{data given parameter}) \cdot P(\text{parameter})}{\int P(\text{data given parameter}) \cdot P(\text{parameter})}
\end{aligned}\]</span></p>
<p>The top of this equation is rather straightforward, you simply multiply the distribution you believe your data is derived from by the distribution you believe the true parameter of interest follows. E.g. the density function of the normal distribution multiplied by the density function of another normal representing the distribution of the mean of the data’s distribution. The problem comes with the integral below. Since it’s integrating over the entire possible space of the parameter of interest it is simply a constant, but finding what constant it is is tricky. Even if you did some optimization method to find it, you still have the problem that most of the math you would want to do with the posterior distribution involves taking an integral over the same values again so just having the normalizing constant is not of much help. <label for="tufte-mn-garfnxzvat" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-tufte-mn-garfnxzvat" class="margin-toggle"><span class="marginnote">One of my mottos in life is, ‘if the math is at all hard, just simulate it’, which turns out to be rather helpful in a lot of statistics.</span></p>
<p>What MCMC does to solve this is, using the numerator of that posterior (also known as the un-normalized posterior), it generates data that fits the distribution, once you have a bunch of samples of data from a given distribution you can do almost any inference you would desire. Want to know how often you would see parameter values greater than <span class="math inline">\(p\)</span>? Just take the samples MCMC generated for you and count how many were above <span class="math inline">\(p\)</span> and divide by the total number of samples.</p>
</div>
<div id="how-does-mcmc-work" class="section level2">
<h2>How does MCMC work?</h2>
<p>There are a few ways of performing MCMC and the way I am going to demonstrate here (Metropolis-Hastings) is not the state of the art,<label for="tufte-mn-kxwnarvzod" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-tufte-mn-kxwnarvzod" class="margin-toggle"><span class="marginnote">For a fantastic overview of the more cutting edge Hamiltonian Monte-Carlo methods, I recomend taking a look at the lecture notes for my class <a href="https://github.com/fonnesbeck/Bios8366/blob/master/notebooks/Section4_4-Hamiltonian-Monte-Carlo.ipynb">on github.</a> </span> however, it is the most straightforward and intuitive method of MCMC and I think is valuable to understand. Essentially the algorithm works like this (I am going to demonstrate for a single parameter but the concept extends to multiple dimensions with the exact same logic):</p>
<ol style="list-style-type: decimal">
<li>Choose a random initial value of the parameter of interest <span class="math inline">\(\theta_0\)</span></li>
<li>Calculate the value of your un-normalized posterior multiplied by this <span class="math inline">\(\theta_0\)</span> and remember it (<span class="math inline">\(P_0\)</span>).</li>
<li>Randomly jump from <span class="math inline">\(\theta_0\)</span> to some new value nearby, <span class="math inline">\(\theta_1\)</span>.</li>
<li>Calculate the value of your un-normalized posterior at this new location, <span class="math inline">\(P_1\)</span>.</li>
<li>Choose to move to <span class="math inline">\(\theta_1\)</span> with probability proportional to how much better or worse <span class="math inline">\(P_1\)</span> is than <span class="math inline">\(P_0\)</span>, if you don’t move make your next <span class="math inline">\(\theta\)</span> a duplicate of <span class="math inline">\(\theta_0\)</span>.</li>
<li>Repeat steps 3-5 for a while, recording each step’s <span class="math inline">\(\theta_i\)</span>.</li>
<li>Treat all of your recorded <span class="math inline">\(\theta\)</span>s as observations from the posterior distribution and relish in the wonderful bayesianness of this whole endeavor.</li>
</ol>
<p>Like all good algorithms there are knobs to turn. For instance, how big of a jump your algorithm takes at any given step is very important. Too large and it will almost never be accepted and the algorithm will take forever to converge to the posterior, too small and the steps nearby each other will be too correlated and the algorithm will focus too much on a given area of the posterior. For metropolis hastings the sweet spot seems to be making your jumps distributed widely enough to have roughly 23% of the proposals accepted. <label for="tufte-mn-mjtucwqllm" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-tufte-mn-mjtucwqllm" class="margin-toggle"><span class="marginnote"><a href="http://probability.ca/jeff/ftpdir/mylene2.pdf">Optimal Acceptance Rates for Metropolis Algorithms:Moving Beyond 0.234</a> is a good paper to read if you’re interested in how these things are calculated and why that single number is still not perfect.</span></p>
<p>However, I’ve just spewed a bunch of words and complex math terms at you, and I had the same thing done to me many times before I truly understood the algorithm. What got me to understand it? Playing with the knobs and seeing how it works. I’ve tried to distill the algorithm to its fundamental parts below for you to tinker with yourself.</p>
</div>
<div id="problem-setup" class="section level2">
<h2>Problem Setup</h2>
<p>Let’s say you have a bird feeder on your porch and you have two bird species that visit it: a Cardinal <label for="tufte-mn-xbzgllhgzf" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-tufte-mn-xbzgllhgzf" class="margin-toggle"><span class="marginnote"><a href="https://www.allaboutbirds.org/guide/Northern_Cardinal/id"><img src="https://www.allaboutbirds.org/guide/PHOTO/LARGE/northern_cardinal_1.jpg" /></a> Cardinal</span> and a Carolina Wren<label for="tufte-mn-ykjnoncoxw" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-tufte-mn-ykjnoncoxw" class="margin-toggle"><span class="marginnote"><a href="https://www.allaboutbirds.org/guide/Carolina_Wren/id"><img src="https://www.allaboutbirds.org/guide/PHOTO/LARGE/carolina_wren_glamor.jpg" /></a> Carolina Wren</span>. You have noticed that there tend to be a lot of seeds spilled on the ground around the feeder. This annoys you because you spent a lot of money on those seeds and those ungrateful birds are wasting a lot of them. You decide to Google the problem and find a handy website that lays out the seed spilling habits of both the Cardinal and the Carolina Wren. This site says that the Cardinal’s seed spillage amount per feeding follows a normal distribution with a mean of 65 and a standard deviation of 10, and the Carolina Wren’s happen to follow a normal distribution with a mean of 85 seeds and a standard deviation of 10 as well. <label for="tufte-mn-frgydnfsys" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-tufte-mn-frgydnfsys" class="margin-toggle"><span class="marginnote">Sometimes the birds carry seeds with them and thus can possibly spill negative seeds, so this truly is normal…</span></p>
<p>The thing is, you want to know what the makeup of your bird spillers are so you can know which bird species to be more mad at. You put a scale beneath the feeder that can automatically measure the spillage from a single feeding. <label for="tufte-mn-umldlfxhrx" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-tufte-mn-umldlfxhrx" class="margin-toggle"><span class="marginnote">It’s a single bird feeder so only one bird feeds at a time.</span> So you have data on how much seed was spilled at each feeding, and you know the distribution of spillage for each bird species, but you have a job so you can’t actually sit there and watch the birds feed, plus you understand that the real distribution of birds feeding probably is just that, a distribution. What can you do? You can do Bayesian statistics!</p>
<p>The data likelihood is a normal mixture: <span class="math inline">\(L(\delta|\text{seeds}) = \delta N(\text{Cardinal}) + (1 - \delta)N(\text{Carolina Wren})\)</span></p>
<p>You don’t have any real idea about which bird will be more likely so your prior is a uniform distribution over the range 0 and 1 (<span class="math inline">\(U(0,1)\)</span>).</p>
<p><label for="tufte-mn-cvskdvppfw" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-tufte-mn-cvskdvppfw" class="margin-toggle"><span class="marginnote">Turns out they actually make <a href="https://www.uniquebirdhouseboutique.com/category/bird-accessories/seed-catchers-birdseed-tray/">products to catch spilled seeds</a> so this convoluted example isn’t too crazy. <img src="https://www.uniquebirdhouseboutique.com/wp-content/uploads/2013/04/seedhoop.jpg" /></span></p>
<p>Your proposal distribution is another random uniform with a width of 2, aka <span class="math inline">\(U(-1,1)\)</span>.</p>
<p>You can fiddle with the sliders below to change the true mean of seeds dropped by two birds, the true percentage of times the Cardinal is at the feeder, the size of your observation data and how wide your proposals are. Play around with the dials to see how the algorithm reacts. The plot below will show the real-time results of MCMC running including the current 95% credible interval (aka what people think a confidence interval is) and the acceptance rate. The algorithms proposed values for the proportion that are rejected are displayed as red circles.</p>
<link rel="stylesheet" href="/js/mcmcExplainer/style.css">
<div class="fullwidth">
<div id="data_plot">

</div>
<div id="data_histogram">

</div>
<div id="sample_trace">

</div>
<div id="sample_histogram">

</div>
<div id="trace_viz">

</div>
</div>
<script src="/js/mcmcExplainer/bundle.js"></script>
<p>Here are a few experiments you may want to run to explore the algorithm’s behavior:</p>
<ul>
<li>Can you fool the credible interval? Try setting the true means of the two birds very close and your number of observations really low, intuitively this will be much harder to discern between each bird in the data.</li>
<li>See how increasing the number of observations results in a skinnier posterior.</li>
<li>If you’re really adventurous, try forking the code for this simulation from <a href="https://github.com/nstrayer/mcmcDemo">github</a> and switching the prior to a beta distribution in <a href="https://github.com/nstrayer/mcmcDemo/blob/master/src/main.js#L27-L28">this file</a>. The beta will allow you to give opinionated priors about the distribution of the parameter of interest, but sometimes at a cost of accuracy.</li>
</ul>
<p><strong>Still confused?</strong> MCMC is non-trivial to say the least, so don’t feel bad if you don’t get it. Please let me know in the comments or on <a href="https://www.twitter.com/NicholasStrayer">twitter</a> what is confusing so I can try and make this explanation better!</p>
</div>

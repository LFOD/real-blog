<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Algorithms on Live Free or Dichotomize</title>
    <link>/tags/algorithms/</link>
    <description>Recent content in Algorithms on Live Free or Dichotomize</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 24 Dec 2017 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/algorithms/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Leveraging uncertainty information from deep neural networks for disease detection - a summary</title>
      <link>/2017/12/24/leveraging-uncertainty-information-from-deep-neural-networks-for-disease-detection---a-summary/</link>
      <pubDate>Sun, 24 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/12/24/leveraging-uncertainty-information-from-deep-neural-networks-for-disease-detection---a-summary/</guid>
      <description>As a biostatistician in the deep learning world I have the awkward task of balancing the dogma of statistics (everything is uncertain) along with the alluring success of some of the newest crazy complex neural network architectures. Going onto any Kaggle competition or new paper ⊕Such as the popular but arguably flawed paper on diagnosing from radiological screens from Andrew Ng et al. you will see models with millions of parameters performing seemingly magical tasks on data of all kinds.</description>
    </item>
    
    <item>
      <title>LSTM neural nets as told by baseball</title>
      <link>/2017/11/08/lstm-neural-nets-as-told-by-baseball/</link>
      <pubDate>Wed, 08 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/11/08/lstm-neural-nets-as-told-by-baseball/</guid>
      <description>Currently I am studying for my qualifying exams on which the topic is “using deep neural networks for classification of time series data.” One extremely popular neural net architecture for doing this is the LSTM, or Long Short Term Memory model. The LSTM is a relatively recent advance ⊕Introduced in 1997 by Hochreiter and Schmidhuber in the the class of networks known as Recurrent Neural Networks (RNNs).
My main problem when I was learning about LSTMs was the horrifically mathematical way the are first described.</description>
    </item>
    
    <item>
      <title>MCMC and the case of the spilled seeds</title>
      <link>/2017/10/14/mcmc-and-the-case-of-the-spilled-seeds/</link>
      <pubDate>Sat, 14 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/10/14/mcmc-and-the-case-of-the-spilled-seeds/</guid>
      <description>Earlier this month I did a post on simulated annealing, an algorithm that I learned in the class I’m currently taking: Advanced Statistical Computing taught by Chris Fonnesbeck here at Vanderbilt. Recently we have moved on to more traditionally “statistics” algorithms, and one that Dr. Fonnesbeck is particularly well versed in, Markov-Chain Monte-Carlo (or MCMC). ⊕Dr. Fonnesbeck is the creator of the very popular python library pymc. In this post I hope to elucidate what MCMC is, why we need it, and show how one particular way of doing it (the metropolis hastings sampler) works.</description>
    </item>
    
    <item>
      <title>The traveling metallurgist</title>
      <link>/2017/09/25/the-traveling-metallurgist/</link>
      <pubDate>Mon, 25 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/09/25/the-traveling-metallurgist/</guid>
      <description>TLDR: I made a thing that moves and has sliders and stuff. It’s down here.
Currently I am taking a class titled “Advanced Statistical Computing” taught here at Vanderbilt by Chris Fonnesbeck. The class is a fantastic whirlwind tour so far of some common optimization algorithms used in statistical computing. One of the algorithms I have found particularly fascinating is the “simulated annealing” algorithm. ⊕The whole algorithm is explained much better than I can do in the freely available Jupyter notebook for the lecture.</description>
    </item>
    
  </channel>
</rss>
<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Visualization on Live Free or Dichotomize</title>
    <link>/tags/visualization/</link>
    <description>Recent content in Visualization on Live Free or Dichotomize</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 14 Oct 2017 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/visualization/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>MCMC and the case of the spilled seeds</title>
      <link>/2017/10/14/mcmc-and-the-case-of-the-spilled-seeds/</link>
      <pubDate>Sat, 14 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/10/14/mcmc-and-the-case-of-the-spilled-seeds/</guid>
      <description>Earlier this month I did a post on simulated annealing, an algorithm that I learned in the class I’m currently taking: Advanced Statistical Computing taught by Chris Fonnesbeck here at Vanderbilt. Recently we have moved on to more traditionally “statistics” algorithms, and one that Dr. Fonnesbeck is particularly well versed in, Markov-Chain Monte-Carlo (or MCMC). ⊕Dr. Fonnesbeck is the creator of the very popular python library pymc. In this post I hope to elucidate what MCMC is, why we need it, and show how one particular way of doing it (the metropolis hastings sampler) works.</description>
    </item>
    
    <item>
      <title>The traveling metallurgist</title>
      <link>/2017/09/25/the-traveling-metallurgist/</link>
      <pubDate>Mon, 25 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/09/25/the-traveling-metallurgist/</guid>
      <description>TLDR: I made a thing that moves and has sliders and stuff. It’s down here.
Currently I am taking a class titled “Advanced Statistical Computing” taught here at Vanderbilt by Chris Fonnesbeck. The class is a fantastic whirlwind tour so far of some common optimization algorithms used in statistical computing. One of the algorithms I have found particularly fascinating is the “simulated annealing” algorithm. ⊕The whole algorithm is explained much better than I can do in the freely available Jupyter notebook for the lecture.</description>
    </item>
    
    <item>
      <title>The Exponential Power Series</title>
      <link>/2017/08/14/the-exponential-power-series/</link>
      <pubDate>Mon, 14 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/08/14/the-exponential-power-series/</guid>
      <description>I am a big fan of the Poisson distribution, there’s something about its simplicity and elegance (I really like \(\lambda\)s) that makes it way easier to deal with than some monstrosity like the gamma or normal distribution.
Another thing I am a big fan of is the book I am currently reading: Surely You’re Joking, Mr Feynman. In one of the chapters, Dr. Feynman discusses his interest in the power series expansion of \(e^x\), and how efficient it is.</description>
    </item>
    
  </channel>
</rss>
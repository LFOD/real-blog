<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Visualization on Live Free or Dichotomize</title>
    <link>/tags/visualization/</link>
    <description>Recent content in Visualization on Live Free or Dichotomize</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 12 Feb 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/visualization/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>The United States of Seasons</title>
      <link>/2018/02/12/the-united-states-of-seasons/</link>
      <pubDate>Mon, 12 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/02/12/the-united-states-of-seasons/</guid>
      <description>The other day my girlfriend and I were talking about places we would like to live after grad school and one of the things that got brought up was how ‘seasonal’ the location is. She grew up in Long Beach, California, which essentially has no seasons, whereas I grew up near Ann Arbor, Michigan which very much has seasons. This got me to thinking: what does the country look like in the context of its seasonality?</description>
    </item>
    
    <item>
      <title>A year as told by fitbit</title>
      <link>/2017/12/27/a-year-as-told-by-fitbit/</link>
      <pubDate>Wed, 27 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/12/27/a-year-as-told-by-fitbit/</guid>
      <description>I managed to wear a fitbit the entirety of 2017, this is exciting for a few reasons: one I have commitment problems, and two: it’s a lot of data that I have to play with. While fitbit’s app has some nice pretty graphs, they make it rather hard to actually dump all of your data into something nice like a csv.
Over the summer I worked on a project with Jeff Leek at the Johns Hopkins Data Science Lab to crack open this fitbit data resource and get out nice tidy csvs of your data.</description>
    </item>
    
    <item>
      <title>MCMC and the case of the spilled seeds</title>
      <link>/2017/10/14/mcmc-and-the-case-of-the-spilled-seeds/</link>
      <pubDate>Sat, 14 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/10/14/mcmc-and-the-case-of-the-spilled-seeds/</guid>
      <description>Earlier this month I did a post on simulated annealing, an algorithm that I learned in the class I’m currently taking: Advanced Statistical Computing taught by Chris Fonnesbeck here at Vanderbilt. Recently we have moved on to more traditionally “statistics” algorithms, and one that Dr. Fonnesbeck is particularly well versed in, Markov-Chain Monte-Carlo (or MCMC). ⊕Dr. Fonnesbeck is the creator of the very popular python library pymc. In this post I hope to elucidate what MCMC is, why we need it, and show how one particular way of doing it (the metropolis hastings sampler) works.</description>
    </item>
    
    <item>
      <title>The traveling metallurgist</title>
      <link>/2017/09/25/the-traveling-metallurgist/</link>
      <pubDate>Mon, 25 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/09/25/the-traveling-metallurgist/</guid>
      <description>TLDR: I made a thing that moves and has sliders and stuff. It’s down here.
Currently I am taking a class titled “Advanced Statistical Computing” taught here at Vanderbilt by Chris Fonnesbeck. The class is a fantastic whirlwind tour so far of some common optimization algorithms used in statistical computing. One of the algorithms I have found particularly fascinating is the “simulated annealing” algorithm. ⊕The whole algorithm is explained much better than I can do in the freely available Jupyter notebook for the lecture.</description>
    </item>
    
    <item>
      <title>The Exponential Power Series</title>
      <link>/2017/08/14/the-exponential-power-series/</link>
      <pubDate>Mon, 14 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/08/14/the-exponential-power-series/</guid>
      <description>I am a big fan of the Poisson distribution, there’s something about its simplicity and elegance (I really like \(\lambda\)s) that makes it way easier to deal with than some monstrosity like the gamma or normal distribution.
Another thing I am a big fan of is the book I am currently reading: Surely You’re Joking, Mr Feynman. In one of the chapters, Dr. Feynman discusses his interest in the power series expansion of \(e^x\), and how efficient it is.</description>
    </item>
    
  </channel>
</rss>